# Alina MirÃ©t Shah  
*Cornell University Â· B.A. Computer Science*  

Building reproducible evaluation and interpretability systems for reliable, transparent AI.

---

## ðŸ”¬ Research Statement
**[Triangulating AI Reliability: Evidence, Mechanisms, and Human Judgment](https://github.com/amshah1022/ai-reliability-agenda/blob/5c996fb8d73703d2632b70a5b8f6aa240339ef27/Triangulating_AI_Reliability___Statement_of_Direction__Alina_Shah_.pdf)**  
Outlines my direction toward a reliability layer for AI safety, integrating evidence-grounded evaluation, mechanistic interpretability, and human/rubric adjudication.

---

## ðŸ’¡ Current Projects
**Truth Layer** â€“ Evidence-grounded evaluation framework that defines truthfulness as alignment with verifiable evidence.  
**MedSim AI** â€“ Rubric-anchored dialogic feedback chatbot (Cornellâ€“Yaleâ€“UCSF) for consistent medical-education feedback.  
**ProbeEng (LAISR)** â€“ Ensemble of 1k+ probes mapping reliability signals across layers and token positions.  

---

## ðŸ§© Skills & Focus
- Large Language Model Evaluation  
- Interpretability (TransformerLens, Probes)  
- Responsible / Reliable AI  
- Python Â· PyTorch Â· Pandas Â· NLP Pipelines  

---

## ðŸ“„ Documents
- [Resume (PDF)](https://github.com/amshah1022/amshah1022.github.io/blob/85ba0fd98df482c65aaa8af95de51ae30cefaea5/Alina_Miret_Shah_Resume.pdf)

---

## ðŸ”— Links
- [GitHub](https://github.com/amshah1022)  
- [LinkedIn](https://linkedin.com/in/alinamshah)  
- [Email](mailto:alina.shah1022@gmail.com)  

---

## ðŸ§  About Me
Iâ€™m an AI researcher focused on developing auditable, evidence-based evaluation systems that make model reliability measurable.  
My goal is to bridge mechanistic interpretability, evidence-grounded NLP, and human-aligned evaluation to build transparent AI pipelines ready for oversight and safety-critical domains.  

---

_Last updated October 2025_

